{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a498e35a",
   "metadata": {},
   "source": [
    "# INGEST INFOMEDIA DATA\n",
    "\n",
    "The data to ingest must have been prepared beforehand with the notebook \"01_Prepare_Infomedia_Data.ipnyb\" of the series. Indeed, it needs its output file \"INFOMEDIA DEDUPLICATED.csv\" as a source.\n",
    "\n",
    "This notebook assumes that you have an Elastic Search (ES) engine installed.\n",
    "\n",
    "The data ingested in ES is intended to be used with the datascapes from this repository: https://github.com/jacomyma/add-datascapes\n",
    "\n",
    "*Authors: Snorre Ralund, Mathieu Jacomy, Anders Munk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3cdff",
   "metadata": {},
   "source": [
    "## How to use\n",
    "1. Edit the settings below\n",
    "2. Run all the cells from the menu: Cell > Run all. **It can take a long time.**\n",
    "3. It's over when the last cell outputs \"Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35419c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {}\n",
    "\n",
    "# SOURCE FILE\n",
    "\n",
    "# It should already be named like this from \"01_Prepare_Infomedia_Data.ipnyb\"\n",
    "settings['infomedia_deduplicated_source'] = 'INFOMEDIA DEDUPLICATED.csv'\n",
    "\n",
    "\n",
    "# ELASTIC SEARCH (ES) CONFIG\n",
    "\n",
    "# URL of the ES node where to ingest the data\n",
    "settings['es_URL'] = 'http://elastic:username@password:port'\n",
    "\n",
    "# ES user\n",
    "settings['es_auth_user'] = 'username'\n",
    "\n",
    "# ES password\n",
    "settings['es_auth_password'] = 'password'\n",
    "\n",
    "# Index name\n",
    "settings['index_name'] = 'infomedia-test'\n",
    "\n",
    "\n",
    "# OTHER SETTINGS\n",
    "\n",
    "# Do you want to delete and respawn the index?\n",
    "# If set to False, it just ingests more data.\n",
    "settings['reset_all'] = True\n",
    "\n",
    "# How many documents to ingest at once?\n",
    "settings['batch_size'] = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037f8b6",
   "metadata": {},
   "source": [
    "**The rest of the script does not require your intervention, except for checking that no errors occur.**\n",
    "\n",
    "Although you are welcome to change it! That is why it is commented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary stuff\n",
    "!conda install elasticsearch -y\n",
    "#!conda install elasticsearch_dsl -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index settings (mappings, essentially)\n",
    "# You do not need to change that\n",
    "es_index_settings = {\n",
    "    \"index\": {\"number_of_replicas\": 2},\n",
    "    \"analysis\": {\n",
    "        \"filter\": {\n",
    "            \"ngram_filter\": {\n",
    "                \"type\": \"edge_ngram\",\n",
    "                \"min_gram\": 2,\n",
    "                \"max_gram\": 15,\n",
    "            },\n",
    "        },\n",
    "        \"analyzer\": {\n",
    "            \"ngram_analyzer\": {\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\", \"ngram_filter\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "es_index_mappings = {\n",
    "    \"properties\": {\n",
    "        \"duid\": {\"type\": \"keyword\"},\n",
    "        \"publishdate\": {\"type\": \"date\"},\n",
    "        \"sourcename\": {\"type\": \"keyword\"},\n",
    "        \"year\": {\"type\": \"integer\"},\n",
    "        \"heading\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"standard\",\n",
    "            \"fields\": {\n",
    "                \"keyword\": {\"type\": \"keyword\"},\n",
    "                \"ngrams\": {\"type\": \"text\", \"analyzer\": \"ngram_analyzer\"},\n",
    "            },\n",
    "        },\n",
    "        \"full_text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"standard\",\n",
    "            \"fields\": {\n",
    "                \"ngrams\": {\"type\": \"text\", \"analyzer\": \"ngram_analyzer\"},\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde07987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Infomedia raw data\n",
    "# You can check what the data look like below.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(settings['infomedia_deduplicated_source']).replace({np.nan: None})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_client = Elasticsearch(\n",
    "    settings['es_URL'],\n",
    "    basic_auth=[settings['es_auth_user'], settings['es_auth_password']],\n",
    ")\n",
    "\n",
    "if settings['reset_all']:\n",
    "    es_client.indices.delete(index=settings['index_name'], ignore_unavailable=True)\n",
    "    es_client.indices.create(index=settings['index_name'], settings=es_index_settings, mappings=es_index_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ea6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve documents already indexed\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "\n",
    "es_client = Elasticsearch(\n",
    "    \"http://elastic:9pSJ5siACNtVAUwB2mj8@10.92.0.111:9200\",\n",
    "    basic_auth=[\"elastic\", \"9pSJ5siACNtVAUwB2mj8\"],\n",
    ")\n",
    "\n",
    "s = Search(using=es_client, index=settings['index_name']) \\\n",
    "    .query(\"match_all\") \\\n",
    "    .source([\"duid\"])\n",
    "\n",
    "response = s.execute()\n",
    "\n",
    "indexed_documents = set()\n",
    "for hit in s.params(scroll=\"20m\").scan():\n",
    "    indexed_documents.add(hit['duid'])\n",
    "\n",
    "print(\"{} documents already in the index.\".format(len(indexed_documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1558b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify documents to index\n",
    "\n",
    "to_index = []\n",
    "for index, row in df.iterrows():\n",
    "    if row['duid'] not in indexed_documents:\n",
    "        to_index.append(row)\n",
    "print(\"{} documents to index yet.\".format(len(to_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920cbfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to monitor what are the remaining documents.\n",
    "# If so, set the condition to True just below:\n",
    "if False:\n",
    "    for row in to_index:\n",
    "        print(row['duid']+\" \"+row[\"heading\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df73b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "while len(to_index)>0:\n",
    "\n",
    "    # Build a batch\n",
    "    batch = []\n",
    "    while len(to_index)>0 and len(batch)<settings['batch_size']:\n",
    "        batch.append(to_index.pop())\n",
    "    \n",
    "    # Inject the batch\n",
    "    actions = []\n",
    "    for row in batch:\n",
    "        action = {\"index\": {\"_index\": settings['index_name'], \"_id\": row[\"duid\"]}}\n",
    "        doc = {\n",
    "            \"duid\": row[\"duid\"],\n",
    "            \"publishdate\": row[\"publishdate\"],\n",
    "            \"sourcename\": row[\"sourcename\"],\n",
    "            \"year\": int(row[\"year\"]),\n",
    "            \"heading\": row[\"heading\"],\n",
    "            \"text\": row[\"clean_text\"],\n",
    "        }\n",
    "        actions.append(json.dumps(action))\n",
    "        actions.append(json.dumps(doc))\n",
    "    result = es_client.bulk(body=\"\\n\".join(actions), request_timeout=100)\n",
    "    if result['errors']:\n",
    "        for r in result['items']:\n",
    "            if 'index' in r and '_id' in r['index'] and 'error' in r['index']:\n",
    "                if 'type' in r['index']['error'] and 'reason' in r['index']['error']:\n",
    "                    print(\"Insertion error for item {}: {}. Reason: {}\".format(r['index']['_id'], r['index']['error']['type'], r['index']['error']['reason']))\n",
    "                else:\n",
    "                    print(\"Insertion error for item {}: {}\".format(r['index']['_id'], r['index']['error']))\n",
    "    print(\"{} documents to index.\".format(len(to_index)))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f33c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
