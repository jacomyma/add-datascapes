{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4639d805",
   "metadata": {},
   "source": [
    "# PREPARE INFOMEDIA DATA\n",
    "\n",
    "Prepares the Infomedia: extract a network of named entities from original data.\n",
    "\n",
    "*Authors: Snorre Ralund, Mathieu Jacomy, Anders Munk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc00757",
   "metadata": {},
   "source": [
    "## How to use\n",
    "1. Edit the settings below\n",
    "2. Run each cell after another, and check that there are no issues\n",
    "\n",
    "In case your process is long because your data is big, you may have to run all in multiple session. As you will see in the code, there are a few checkpoints where the data is saved, and where you can go back directly. If you do that, you still need to run the settings and the section 1. with all the installs and imports.\n",
    "\n",
    "**THIS IS A VERY LONG PROCESS.** Extracting data with DaCy requires computing power, and it will take a long time if you have many documents. That bottleneck happens in step 11. It also requires **a lot of disk space** (from Gb to hunderds of Gb, depending on your corpus size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff65ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:29:46.662955Z",
     "iopub.status.busy": "2022-09-29T12:29:46.662746Z",
     "iopub.status.idle": "2022-09-29T12:29:46.668281Z",
     "shell.execute_reply": "2022-09-29T12:29:46.667299Z",
     "shell.execute_reply.started": "2022-09-29T12:29:46.662935Z"
    }
   },
   "outputs": [],
   "source": [
    "settings = {}\n",
    "\n",
    "# Which file contains the raw data?\n",
    "settings['source file'] = \"Infomedia raw data SAMPLE.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e0319e",
   "metadata": {},
   "source": [
    "## 1. Install and load DaCy and other libraries\n",
    "\n",
    "DaCy is the library we use to extract data from Danish text. It is kind of the Danish version of SpaCy. You may have to restart the kernel after installing it. We used the large model, but you can use a smaller one for more performance a less accuracy, if your machine is limited. Please check DaCy's documentation in case install problem. Relevant resources:\n",
    "* [DaCy on PyPi](https://pypi.org/project/dacy/)\n",
    "* [DaCy on SpaCy Universe](https://spacy.io/universe/project/dacy)\n",
    "* [DaCy's GitHub repository](https://github.com/centre-for-humanities-computing/DaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ca6a9-0536-4e79-9c9b-44037e8a45b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:29:46.669346Z",
     "iopub.status.busy": "2022-09-29T12:29:46.669158Z",
     "iopub.status.idle": "2022-09-29T12:29:53.353313Z",
     "shell.execute_reply": "2022-09-29T12:29:53.351648Z",
     "shell.execute_reply.started": "2022-09-29T12:29:46.669329Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install dacy[all] --quiet\n",
    "! pip install dacy[large] --quiet\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d04ccc-bd4b-4841-a078-f84cc7020d0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:29:53.356873Z",
     "iopub.status.busy": "2022-09-29T12:29:53.356237Z",
     "iopub.status.idle": "2022-09-29T12:29:56.695197Z",
     "shell.execute_reply": "2022-09-29T12:29:56.692943Z",
     "shell.execute_reply.started": "2022-09-29T12:29:53.356810Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: on one system we had the following error in the next cell:\n",
    "# ContextualVersionConflict: (click 8.1.3 (/opt/conda/lib/python3.10/site-packages), Requirement.parse('click<8.1.0'), {'spacy'})\n",
    "# If that happens, uncomment and run the line below:\n",
    "# !pip install 'click<8.1.0' --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13ed5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:29:56.711788Z",
     "iopub.status.busy": "2022-09-29T12:29:56.709472Z",
     "iopub.status.idle": "2022-09-29T12:29:57.904705Z",
     "shell.execute_reply": "2022-09-29T12:29:57.902758Z",
     "shell.execute_reply.started": "2022-09-29T12:29:56.711735Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just a check of the models available\n",
    "import dacy\n",
    "print(\"List of models available:\")\n",
    "for model in dacy.models():\n",
    "    print(\"- \"+model)\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551abf0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:29:57.911553Z",
     "iopub.status.busy": "2022-09-29T12:29:57.910778Z",
     "iopub.status.idle": "2022-09-29T12:31:18.362904Z",
     "shell.execute_reply": "2022-09-29T12:31:18.361917Z",
     "shell.execute_reply.started": "2022-09-29T12:29:57.911482Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the large model (may be long)\n",
    "import dacy\n",
    "nlp = dacy.load('large')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3158a3",
   "metadata": {},
   "source": [
    "We also need to install a few other things like [NLTK](https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a802a-ffe9-4b46-8b78-c553e4f657c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:32:18.918486Z",
     "iopub.status.busy": "2022-09-29T12:32:18.917815Z",
     "iopub.status.idle": "2022-09-29T12:32:44.052844Z",
     "shell.execute_reply": "2022-09-29T12:32:44.050746Z",
     "shell.execute_reply.started": "2022-09-29T12:32:18.918429Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install nltk --quiet\n",
    "! python -m nltk.downloader stopwords\n",
    "! python -m nltk.downloader punkt\n",
    "! pip install networkx --quiet\n",
    "! pip install gensim -U --quiet\n",
    "! pip install python-louvain --quiet\n",
    "! pip install sklearn --quiet\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663aee3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:32:49.221855Z",
     "iopub.status.busy": "2022-09-29T12:32:49.221140Z",
     "iopub.status.idle": "2022-09-29T12:32:49.741853Z",
     "shell.execute_reply": "2022-09-29T12:32:49.740866Z",
     "shell.execute_reply.started": "2022-09-29T12:32:49.221797Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Other imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "import nltk\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from community import community_louvain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123b07b3",
   "metadata": {},
   "source": [
    "## 2. Clean the data\n",
    "\n",
    "The source material needs some cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ab026-4efb-45fc-a755-b93ecbb77ad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:32:53.196270Z",
     "iopub.status.busy": "2022-09-29T12:32:53.195574Z",
     "iopub.status.idle": "2022-09-29T12:32:55.346984Z",
     "shell.execute_reply": "2022-09-29T12:32:55.346259Z",
     "shell.execute_reply.started": "2022-09-29T12:32:53.196214Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Source file\n",
    "df = pd.read_csv(settings['source file'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5cd3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:32:55.355576Z",
     "iopub.status.busy": "2022-09-29T12:32:55.355399Z",
     "iopub.status.idle": "2022-09-29T12:32:55.376681Z",
     "shell.execute_reply": "2022-09-29T12:32:55.376056Z",
     "shell.execute_reply.started": "2022-09-29T12:32:55.355560Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add year column\n",
    "df['year'] = df.publishdate.apply(lambda x: int(x.split('-')[0]))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff88920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:32:55.378270Z",
     "iopub.status.busy": "2022-09-29T12:32:55.378095Z",
     "iopub.status.idle": "2022-09-29T12:32:55.399405Z",
     "shell.execute_reply": "2022-09-29T12:32:55.398115Z",
     "shell.execute_reply.started": "2022-09-29T12:32:55.378254Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create cleaning function\n",
    "# to fix text error from Infomedia, joining paragraphs with no space\n",
    "import re\n",
    "re_html = re.compile('(?:(?:https?://)?www.[^ ]+)|(?:[^ ]+\\.com[^ ]+)|(?:[^ ]+\\.org[^ ]+)|(?:[^ ]+\\.dk[^ ]+)')\n",
    "re_error = re.compile('(?:(?:[a-zæøå\"][\\.?!])|(?:[A-Z]{2}[\\.?!]))([A-ZÆØÅ0-9])')\n",
    "\n",
    "def clean_text_infomedia_error(text):\n",
    "    if type(text)!=str:\n",
    "        return text,[]\n",
    "    l = re_error.finditer(text)\n",
    "    indices = [m.start(0) for m in l]\n",
    "    if len(indices)==0:\n",
    "        return text,[]\n",
    "    \n",
    "    bits = []\n",
    "    last = 0\n",
    "    for i in indices:\n",
    "        loc = list(text[i:i+3])\n",
    "        dot = 0\n",
    "        for lo in ['.','!','?']:\n",
    "            try:\n",
    "                dot = list(loc).index(lo)\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "        idx = i+dot+1\n",
    "        bits.append(text[last:idx])\n",
    "        last = idx\n",
    "    bits.append(text[last:])\n",
    "    new_text = ' '.join(bits)\n",
    "    text = new_text\n",
    "    # remove links    \n",
    "    links = re_html.findall(text)\n",
    "    for link in set(links):\n",
    "        text = text.replace(link.strip('.'),'LINK')\n",
    "    # make space between sources\n",
    "    sources = re.findall('/[^/ ]{1,}/',text)\n",
    "    for source in sources:\n",
    "        text = text.replace(source,' %s '%source)\n",
    "    return text,links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c37700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:32:55.402314Z",
     "iopub.status.busy": "2022-09-29T12:32:55.401608Z",
     "iopub.status.idle": "2022-09-29T12:32:55.425192Z",
     "shell.execute_reply": "2022-09-29T12:32:55.423962Z",
     "shell.execute_reply.started": "2022-09-29T12:32:55.402261Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test cleaning function (useful for debug and maintenance)\n",
    "text = df.bodytext.sample(1).iloc[0]\n",
    "new_text,links = clean_text_infomedia_error(text)\n",
    "print(\"Links found (URLs): \"+str(new_text.count('LINK')))\n",
    "print(\"Clean text: \"+new_text)\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e218d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:32:55.426867Z",
     "iopub.status.busy": "2022-09-29T12:32:55.426509Z",
     "iopub.status.idle": "2022-09-29T12:33:17.723054Z",
     "shell.execute_reply": "2022-09-29T12:33:17.722140Z",
     "shell.execute_reply.started": "2022-09-29T12:32:55.426834Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean the data (with links removed)\n",
    "Links = []\n",
    "texts = []\n",
    "import tqdm\n",
    "for text in tqdm.tqdm(df.bodytext.values):\n",
    "    text,links = clean_text_infomedia_error(text)\n",
    "    texts.append(text)\n",
    "    Links.append(links)\n",
    "df['clean_text'] = texts\n",
    "df['links'] = Links\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3e929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:33:17.724547Z",
     "iopub.status.busy": "2022-09-29T12:33:17.724331Z",
     "iopub.status.idle": "2022-09-29T12:33:17.879137Z",
     "shell.execute_reply": "2022-09-29T12:33:17.878364Z",
     "shell.execute_reply.started": "2022-09-29T12:33:17.724521Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add heading to full text\n",
    "full = []\n",
    "for i,j in df[['heading','clean_text']].fillna('').values:\n",
    "    full.append(' .\\t'.join([i,j]))\n",
    "df['text'] = full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba332b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:33:17.880262Z",
     "iopub.status.busy": "2022-09-29T12:33:17.880055Z",
     "iopub.status.idle": "2022-09-29T12:33:17.962580Z",
     "shell.execute_reply": "2022-09-29T12:33:17.961891Z",
     "shell.execute_reply.started": "2022-09-29T12:33:17.880245Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the links back in\n",
    "import re\n",
    "with_links = []\n",
    "for text,links in df[['text','links']].values:\n",
    "    if len(links)>0:\n",
    "        for link in links:\n",
    "            text = re.sub('LINK',link,text,count=1)\n",
    "    with_links.append(text)\n",
    "df['full_text'] = with_links\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0f84c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:33:17.963626Z",
     "iopub.status.busy": "2022-09-29T12:33:17.963432Z",
     "iopub.status.idle": "2022-09-29T12:33:17.975442Z",
     "shell.execute_reply": "2022-09-29T12:33:17.974881Z",
     "shell.execute_reply.started": "2022-09-29T12:33:17.963610Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Monitor what the data look like at this stage\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d8435b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:33:26.508723Z",
     "iopub.status.busy": "2022-09-29T12:33:26.508527Z",
     "iopub.status.idle": "2022-09-29T12:33:35.631786Z",
     "shell.execute_reply": "2022-09-29T12:33:35.630080Z",
     "shell.execute_reply.started": "2022-09-29T12:33:26.508706Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the clean data\n",
    "df.to_csv('prep 02 - infomedia cleaned data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99968440",
   "metadata": {},
   "source": [
    "## 3. Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134b902",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:34:01.380336Z",
     "iopub.status.busy": "2022-09-29T12:34:01.379610Z",
     "iopub.status.idle": "2022-09-29T12:34:01.385875Z",
     "shell.execute_reply": "2022-09-29T12:34:01.384735Z",
     "shell.execute_reply.started": "2022-09-29T12:34:01.380283Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the clean data (uncomment if you restart from here)\n",
    "# df = pd.read_csv('prep 02 - infomedia cleaned data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd478c13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:34:01.387737Z",
     "iopub.status.busy": "2022-09-29T12:34:01.387272Z",
     "iopub.status.idle": "2022-09-29T12:36:15.005775Z",
     "shell.execute_reply": "2022-09-29T12:36:15.004908Z",
     "shell.execute_reply.started": "2022-09-29T12:34:01.387704Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "texts = []\n",
    "for duid,text in tqdm.tqdm(df[['duid','full_text']].values):\n",
    "    texts.append((duid,nltk.word_tokenize(text)))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8a9f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:36:15.007278Z",
     "iopub.status.busy": "2022-09-29T12:36:15.007056Z",
     "iopub.status.idle": "2022-09-29T12:36:22.737499Z",
     "shell.execute_reply": "2022-09-29T12:36:22.736732Z",
     "shell.execute_reply.started": "2022-09-29T12:36:15.007257Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the tokenized text\n",
    "pickle.dump(texts,open('prep 03 - infomedia tokenized texts.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8e6e10",
   "metadata": {},
   "source": [
    "## 4. Build document-term matrix\n",
    "We need this for deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb5af0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:36:22.738609Z",
     "iopub.status.busy": "2022-09-29T12:36:22.738403Z",
     "iopub.status.idle": "2022-09-29T12:36:22.742409Z",
     "shell.execute_reply": "2022-09-29T12:36:22.741776Z",
     "shell.execute_reply.started": "2022-09-29T12:36:22.738591Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tokenized text (uncomment if you restart from here)\n",
    "# texts = pickle.load(open('prep 03 - infomedia tokenized texts.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92eb448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:36:22.743333Z",
     "iopub.status.busy": "2022-09-29T12:36:22.743162Z",
     "iopub.status.idle": "2022-09-29T12:41:03.122522Z",
     "shell.execute_reply": "2022-09-29T12:41:03.121555Z",
     "shell.execute_reply.started": "2022-09-29T12:36:22.743317Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make Document Term Matrix\n",
    "def get_ngram(doc,n=2):\n",
    "    grams = doc.copy()\n",
    "\n",
    "    for gram in range(2,n+1):\n",
    "        grams+=['_'.join(doc[i:i+gram]) for i in range(len(doc)+1-gram)]\n",
    "    return grams\n",
    "def docs_to_dtm(docs,max_words=100000,min_count=5,ngram=3):\n",
    "    c = Counter()\n",
    "    bows = [] \n",
    "    for doc in docs:\n",
    "        doc = get_ngram(doc,n=ngram)\n",
    "        d = Counter(doc)\n",
    "        c.update(d)\n",
    "        bows.append(d)\n",
    "    # make index\n",
    "    index = [w for w,count in c.most_common(max_words) if count>min_count]\n",
    "    w2i = {w:num for num,w in enumerate(index)} \n",
    "    \n",
    "    # initialize matrix\n",
    "    X = sp.dok_matrix((len(docs),len(index)), dtype=np.int32)\n",
    "    for num in range(len(bows)):\n",
    "        bow = bows[num]\n",
    "        for w,count in bow.items():\n",
    "            \n",
    "            try:\n",
    "                X[num,w2i[w]]=count\n",
    "            except:\n",
    "                pass\n",
    "    X = X.tocsr()\n",
    "    return index,X\n",
    "%time index,dtm = docs_to_dtm([list(j) for i,j in texts])\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a897b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:41:03.123967Z",
     "iopub.status.busy": "2022-09-29T12:41:03.123754Z",
     "iopub.status.idle": "2022-09-29T12:41:03.338842Z",
     "shell.execute_reply": "2022-09-29T12:41:03.337605Z",
     "shell.execute_reply.started": "2022-09-29T12:41:03.123947Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the matrix\n",
    "pickle.dump([dtm,index],open('prep 04 - dtm index.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89fa79",
   "metadata": {},
   "source": [
    "## 5. Compute TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60b5d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:41:03.339860Z",
     "iopub.status.busy": "2022-09-29T12:41:03.339662Z",
     "iopub.status.idle": "2022-09-29T12:41:03.383374Z",
     "shell.execute_reply": "2022-09-29T12:41:03.382064Z",
     "shell.execute_reply.started": "2022-09-29T12:41:03.339843Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load document-term matrix (uncomment if you restart from here)\n",
    "# dtm,index = pickle.load(open('prep 04 - dtm index.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e32ffe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:41:03.386342Z",
     "iopub.status.busy": "2022-09-29T12:41:03.385644Z",
     "iopub.status.idle": "2022-09-29T12:41:05.384974Z",
     "shell.execute_reply": "2022-09-29T12:41:05.383932Z",
     "shell.execute_reply.started": "2022-09-29T12:41:03.386289Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### transform to TFIDF\n",
    "def dtm_tfidf(dtm):\n",
    "    # Document frequency\n",
    "    df = np.asarray(dtm.sign().sum(axis=0))[0,:]\n",
    "    # Inverse document frequency\n",
    "    idf =-np.log(df/dtm.shape[0])\n",
    "    # Combined term frequence and inverse document frequency\n",
    "    tfidf = dtm.multiply(idf)\n",
    "    return tfidf\n",
    "tfidf = dtm_tfidf(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a58bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:41:05.386037Z",
     "iopub.status.busy": "2022-09-29T12:41:05.385840Z",
     "iopub.status.idle": "2022-09-29T12:41:05.675616Z",
     "shell.execute_reply": "2022-09-29T12:41:05.674603Z",
     "shell.execute_reply.started": "2022-09-29T12:41:05.386020Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save TFIDF\n",
    "pickle.dump(tfidf,open('prep 05 - tfidf.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6dc309",
   "metadata": {},
   "source": [
    "## 6. Compute similarity information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0119b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:41:05.676751Z",
     "iopub.status.busy": "2022-09-29T12:41:05.676553Z",
     "iopub.status.idle": "2022-09-29T12:41:05.680315Z",
     "shell.execute_reply": "2022-09-29T12:41:05.679749Z",
     "shell.execute_reply.started": "2022-09-29T12:41:05.676734Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data (uncomment if you restart from here)\n",
    "# dtm,index = pickle.load(open('prep 04 - dtm index.pkl','rb'))\n",
    "# tfidf = pickle.load(open('prep 05 - tfidf.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c34e19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:53:40.186448Z",
     "iopub.status.busy": "2022-09-29T12:53:40.185728Z",
     "iopub.status.idle": "2022-09-29T12:58:28.473868Z",
     "shell.execute_reply": "2022-09-29T12:58:28.472793Z",
     "shell.execute_reply.started": "2022-09-29T12:53:40.186384Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute and save similarity matrix\n",
    "%time doc2doc = cosine_similarity(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24447271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T12:58:43.628401Z",
     "iopub.status.busy": "2022-09-29T12:58:43.628212Z",
     "iopub.status.idle": "2022-09-29T13:03:33.132507Z",
     "shell.execute_reply": "2022-09-29T13:03:33.131491Z",
     "shell.execute_reply.started": "2022-09-29T12:58:43.628384Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute and save similarity matrix TFIDF\n",
    "%time doc2doc_tfidf = cosine_similarity(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e1a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:03:33.135311Z",
     "iopub.status.busy": "2022-09-29T13:03:33.135083Z",
     "iopub.status.idle": "2022-09-29T13:06:12.098662Z",
     "shell.execute_reply": "2022-09-29T13:06:12.097156Z",
     "shell.execute_reply.started": "2022-09-29T13:03:33.135290Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set similarity to oneself to 0\n",
    "n_docs = doc2doc.shape[0]\n",
    "doc2doc[np.arange(n_docs),np.arange(n_docs)] = 0\n",
    "doc2doc_tfidf[np.arange(n_docs),np.arange(n_docs)] = 0\n",
    "# locate the distribution of closests matches\n",
    "top,top2 = [],[]\n",
    "# tfidf score of the best. \n",
    "top3 = []\n",
    "top4 = []\n",
    "match = []\n",
    "for i in range(doc2doc.shape[0]):\n",
    "    a = doc2doc[i]\n",
    "    a2 = doc2doc_tfidf[i]   \n",
    "    best = a.argsort()[-1]\n",
    "    #match.append((a[best],best))\n",
    "    match.append((a[best],a2[best],best))\n",
    "    \n",
    "    top.append(a[best])\n",
    "    top4.append(a2[best])\n",
    "    best = a2.argsort()[-1]\n",
    "    top3.append(a[best])\n",
    "    top2.append(a2[best])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b28df4-a03f-4dc1-b3a9-94c4e92ddf35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:12.100262Z",
     "iopub.status.busy": "2022-09-29T13:06:12.100040Z",
     "iopub.status.idle": "2022-09-29T13:06:29.709671Z",
     "shell.execute_reply": "2022-09-29T13:06:29.708668Z",
     "shell.execute_reply.started": "2022-09-29T13:06:12.100240Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "pickle.dump(doc2doc,open('prep 06 - similarity matrix.pkl','wb'))\n",
    "pickle.dump(doc2doc_tfidf,open('prep 06 - similarity matrix tfidf.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ef4a7",
   "metadata": {},
   "source": [
    "## 7. Identify duplicates\n",
    "\n",
    "Duplicates form groups. We gather those groups as duplicates of each other and we save this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13fb39f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:29.710802Z",
     "iopub.status.busy": "2022-09-29T13:06:29.710601Z",
     "iopub.status.idle": "2022-09-29T13:06:29.715212Z",
     "shell.execute_reply": "2022-09-29T13:06:29.714514Z",
     "shell.execute_reply.started": "2022-09-29T13:06:29.710785Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data (uncomment if you restart from here)\n",
    "# texts = pickle.load(open('prep 03 - infomedia tokenized texts.pkl','rb'))\n",
    "# doc2doc = pickle.load(open('prep 06 - similarity matrix.pkl','rb'))\n",
    "# doc2doc_tfidf = pickle.load(open('prep 06 - similarity matrix tfidf.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884e79e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:29.716047Z",
     "iopub.status.busy": "2022-09-29T13:06:29.715868Z",
     "iopub.status.idle": "2022-09-29T13:06:37.191648Z",
     "shell.execute_reply": "2022-09-29T13:06:37.190975Z",
     "shell.execute_reply.started": "2022-09-29T13:06:29.716031Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build simiarlity network\n",
    "g = nx.Graph()\n",
    "#thres = 0.965 # threshold for DTM cosine similarity (i.e. unweighed)\n",
    "len_thres = 0.9 # if length of document is less then 90 % percent of the other\n",
    "                # it is not counted as a duplicate\n",
    "#thres2 = 0.99 # \n",
    "tf_cut = 0.93\n",
    "tfidf_cut = 0.91\n",
    "for i in tqdm.tqdm(np.arange(doc2doc.shape[0])):\n",
    "    a = doc2doc[i]\n",
    "    n = sum(map(len,texts[i][1]))\n",
    "    a2 = doc2doc_tfidf[i]\n",
    "    idx = np.arange(len(a))[(a>=tf_cut)&(a2>=tfidf_cut)]\n",
    "    for j in idx:\n",
    "        n2 = sum(map(len,texts[j][1]))\n",
    "        # Account for length difference.\n",
    "        l = sorted([n,n2])\n",
    "        diff = l[0]/l[1]\n",
    "        if i == j or diff<=len_thres:\n",
    "            continue\n",
    "        \n",
    "        g.add_edge(i,j)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479211ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:37.192702Z",
     "iopub.status.busy": "2022-09-29T13:06:37.192478Z",
     "iopub.status.idle": "2022-09-29T13:06:37.207380Z",
     "shell.execute_reply": "2022-09-29T13:06:37.206786Z",
     "shell.execute_reply.started": "2022-09-29T13:06:37.192683Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get articles that do not have duplicates\n",
    "nondupes = set(np.arange(len(texts)))-set(g)\n",
    "print(\"Tokens (nodes):\",len(g),\"Similarities (edges):\",len(g.edges()),\"Non-duplicates: \",len(nondupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923b605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:37.208312Z",
     "iopub.status.busy": "2022-09-29T13:06:37.208128Z",
     "iopub.status.idle": "2022-09-29T13:06:37.256439Z",
     "shell.execute_reply": "2022-09-29T13:06:37.255837Z",
     "shell.execute_reply.started": "2022-09-29T13:06:37.208295Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The components, in the network, are the groups of duplicated articles\n",
    "comps = list(nx.connected_components(g))\n",
    "len(comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a4cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:37.259493Z",
     "iopub.status.busy": "2022-09-29T13:06:37.259269Z",
     "iopub.status.idle": "2022-09-29T13:06:38.005712Z",
     "shell.execute_reply": "2022-09-29T13:06:38.005037Z",
     "shell.execute_reply.started": "2022-09-29T13:06:37.259476Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List articles with duplication information.\n",
    "\n",
    "# What the columns mean:\n",
    "# duid: article id\n",
    "# duplicate: 0 if it has no duplicates or is the \"original\", 1 else. I.e.: 0=keep, 1=remove.\n",
    "# n_dupes: how many articles in the group of duplicates\n",
    "# original: which article we consider the reference (it's arbitrary, but the same for the whole group)\n",
    "# density: how many pairs of articles are duplicates of each other (from 0=0% to 1=100%)\n",
    "\n",
    "# Explanation: if A is dupe of B and B is dupe of C then we see A as dupe of C,\n",
    "#              but the A to C connection might not be considered duplication\n",
    "#              according to the thresholds we used.\n",
    "#              So a density below 1 tells us that such a thing happened.\n",
    "#              That is not a problem, though.\n",
    "\n",
    "dat = [{'duid':texts[i][0],'duplicate':0,'n_dupes':0,'original':texts[i][0]} for i in nondupes]\n",
    "\n",
    "for comp in comps:\n",
    "    # keep the largest.\n",
    "    best = max(comp,key=lambda x: sum(map(len,texts[x][1])))\n",
    "    dupes = set(comp)\n",
    "    dupes.remove(best)\n",
    "    n_dupes = len(dupes) # keep how many duplicate were removed\n",
    "    dens = nx.density(nx.subgraph(g,comp))\n",
    "    dat.append({'duid':texts[best][0],'duplicate':0,'n_dupes':n_dupes,'density':dens})\n",
    "    for i in dupes:\n",
    "        dat.append({'duid':texts[i][0],'duplicate':1,'n_dupes':n_dupes,'original':texts[best][0],'density':dens})\n",
    "    \n",
    "ddf = pd.DataFrame(dat)\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a9d94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:38.006650Z",
     "iopub.status.busy": "2022-09-29T13:06:38.006443Z",
     "iopub.status.idle": "2022-09-29T13:06:38.076736Z",
     "shell.execute_reply": "2022-09-29T13:06:38.076028Z",
     "shell.execute_reply.started": "2022-09-29T13:06:38.006632Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "ids = [i[0] for i in texts]\n",
    "ddf.index = ddf.duid\n",
    "ddf = ddf.loc[ids]\n",
    "ddf = ddf.reset_index(drop=True)\n",
    "ddf.to_csv('prep 07 - infomedia duplicates.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0947f40",
   "metadata": {},
   "source": [
    "## 8. Remove duplicates\n",
    "\n",
    "We produce a file with no duplicates. This does not mean that the deduplicated file is always the right file to use. For instance if you want to count the number of occurrences of an expression, you may want to take into account that some articles have been published multiple times.\n",
    "\n",
    "We will need the deduped file, however. Indeed, as our process looks into co-occurrence, duplicated articles would create artifacts by inflating the co-occurrence of the expressions they contain. Therefore, to improve the data, we use the deduplicated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3eeba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:38.077786Z",
     "iopub.status.busy": "2022-09-29T13:06:38.077599Z",
     "iopub.status.idle": "2022-09-29T13:06:38.080811Z",
     "shell.execute_reply": "2022-09-29T13:06:38.080234Z",
     "shell.execute_reply.started": "2022-09-29T13:06:38.077770Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data (uncomment if you restart from here)\n",
    "# df = pd.read_csv('prep 02 - infomedia cleaned data.csv')\n",
    "# ddf = pd.read_csv('prep 07 - infomedia duplicates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916641fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:38.081706Z",
     "iopub.status.busy": "2022-09-29T13:06:38.081499Z",
     "iopub.status.idle": "2022-09-29T13:06:38.145923Z",
     "shell.execute_reply": "2022-09-29T13:06:38.144933Z",
     "shell.execute_reply.started": "2022-09-29T13:06:38.081684Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "after_dup = df[ddf.duplicate==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806b7e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:38.146787Z",
     "iopub.status.busy": "2022-09-29T13:06:38.146612Z",
     "iopub.status.idle": "2022-09-29T13:06:43.151070Z",
     "shell.execute_reply": "2022-09-29T13:06:43.150112Z",
     "shell.execute_reply.started": "2022-09-29T13:06:38.146771Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "after_dup.to_csv('prep 08 - infomedia deduplicated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e806db",
   "metadata": {},
   "source": [
    "## 9. Detect language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfc1c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:43.152145Z",
     "iopub.status.busy": "2022-09-29T13:06:43.151950Z",
     "iopub.status.idle": "2022-09-29T13:06:43.155353Z",
     "shell.execute_reply": "2022-09-29T13:06:43.154786Z",
     "shell.execute_reply.started": "2022-09-29T13:06:43.152128Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data (uncomment if you restart from here)\n",
    "# texts = pickle.load(open('prep 03 - infomedia tokenized texts.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d42eb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:43.156197Z",
     "iopub.status.busy": "2022-09-29T13:06:43.156022Z",
     "iopub.status.idle": "2022-09-29T13:06:47.884019Z",
     "shell.execute_reply": "2022-09-29T13:06:47.882496Z",
     "shell.execute_reply.started": "2022-09-29T13:06:43.156181Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check stop words\n",
    "import requests\n",
    "stopwords = set(nltk.corpus.stopwords.words('danish'))\n",
    "urls = ['https://gist.githubusercontent.com/berteltorp/0cf8a0c7afea7f25ed754f24cfc2467b/raw/305d8e3930cc419e909d49d4b489c9773f75b2d6/stopord.txt',\n",
    "       'https://raw.githubusercontent.com/stopwords-iso/stopwords-da/master/stopwords-da.txt']\n",
    "stopwords1 = set()\n",
    "for url in urls:\n",
    "    stopwords1.update(set(requests.get(url).text.split()))\n",
    "from spacy.lang.da.stop_words import STOP_WORDS\n",
    "\n",
    "stop = STOP_WORDS|stopwords|stopwords1\n",
    "print(len(stop),len(stopwords1),len(stopwords),len(STOP_WORDS))\n",
    "special_tokens = 'æøå'\n",
    "stop_score = []\n",
    "for d_id,text in texts:\n",
    "    c = Counter(text)\n",
    "    raw = ''.join(text)\n",
    "    count = 0\n",
    "    for i in special_tokens:\n",
    "        count+=raw.count(i)\n",
    "    n = sum(c.values())\n",
    "    match = sum([c[i] for i in stop])\n",
    "    p = match/n\n",
    "    stop_score.append((p,count/len(raw)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e776cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:47.885509Z",
     "iopub.status.busy": "2022-09-29T13:06:47.885274Z",
     "iopub.status.idle": "2022-09-29T13:06:47.898543Z",
     "shell.execute_reply": "2022-09-29T13:06:47.897824Z",
     "shell.execute_reply.started": "2022-09-29T13:06:47.885488Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stops,special = zip(*stop_score)\n",
    "stops = np.array(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea41ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:47.899489Z",
     "iopub.status.busy": "2022-09-29T13:06:47.899320Z",
     "iopub.status.idle": "2022-09-29T13:06:58.651602Z",
     "shell.execute_reply": "2022-09-29T13:06:58.649700Z",
     "shell.execute_reply.started": "2022-09-29T13:06:47.899473Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# language detectors\n",
    "! pip install langdetect\n",
    "from langdetect import detect as detect\n",
    "! pip install language-detector\n",
    "from language_detector import detect_language as detect2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0beca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:06:58.655336Z",
     "iopub.status.busy": "2022-09-29T13:06:58.654643Z",
     "iopub.status.idle": "2022-09-29T13:15:26.896341Z",
     "shell.execute_reply": "2022-09-29T13:15:26.895435Z",
     "shell.execute_reply.started": "2022-09-29T13:06:58.655267Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detect languages\n",
    "dat = []\n",
    "for _,text in tqdm.tqdm(texts):\n",
    "    s = ' '.join(text)\n",
    "    \n",
    "    try:\n",
    "        lan = detect(s)\n",
    "        lan2 = detect2(s)\n",
    "    except:\n",
    "        dat.append({'duid':_})\n",
    "        continue\n",
    "    dat.append({'lan':lan,'lan2':lan2,'duid':_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136ca5b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:15:26.897713Z",
     "iopub.status.busy": "2022-09-29T13:15:26.897482Z",
     "iopub.status.idle": "2022-09-29T13:15:27.077367Z",
     "shell.execute_reply": "2022-09-29T13:15:27.076675Z",
     "shell.execute_reply.started": "2022-09-29T13:15:26.897683Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Integrate data into dataframe\n",
    "import pandas as pd\n",
    "ldf = pd.DataFrame(dat)\n",
    "ldf['special_chr'] = special\n",
    "ldf['stopword_p'] = stops\n",
    "def language_decision(row,stopthres=0.15,special_thres = 0.005):\n",
    "    lan = row['lan']\n",
    "    if lan=='da':\n",
    "        return lan\n",
    "    if row['stopword_p']>=stopthres:\n",
    "        return 'da'\n",
    "    if row['stopword_p']>=(stopthres-0.05) and row['special_chr']>special_thres:\n",
    "        return 'da'\n",
    "    return lan\n",
    "ldf['Language'] = ldf.apply(language_decision,axis=1)\n",
    "ldf = ldf[sorted(ldf.columns)]\n",
    "ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7a8f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:15:27.078433Z",
     "iopub.status.busy": "2022-09-29T13:15:27.078231Z",
     "iopub.status.idle": "2022-09-29T13:15:27.231137Z",
     "shell.execute_reply": "2022-09-29T13:15:27.230431Z",
     "shell.execute_reply.started": "2022-09-29T13:15:27.078415Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "ldf.to_csv('prep 09 - infomedia language detect.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939e0ee",
   "metadata": {},
   "source": [
    "## 10. Remove non-danish documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4227c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:15:27.232145Z",
     "iopub.status.busy": "2022-09-29T13:15:27.231954Z",
     "iopub.status.idle": "2022-09-29T13:15:27.235774Z",
     "shell.execute_reply": "2022-09-29T13:15:27.235234Z",
     "shell.execute_reply.started": "2022-09-29T13:15:27.232128Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data (uncomment if you restart from here)\n",
    "# after_dup = pd.read_csv('prep 08 - infomedia deduplicated.csv')\n",
    "# ldf = pd.read_csv('prep 09 - infomedia language detect.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c4fff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:15:27.236643Z",
     "iopub.status.busy": "2022-09-29T13:15:27.236461Z",
     "iopub.status.idle": "2022-09-29T13:15:27.287225Z",
     "shell.execute_reply": "2022-09-29T13:15:27.286564Z",
     "shell.execute_reply.started": "2022-09-29T13:15:27.236627Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = set()\n",
    "out.update(set(ldf[ldf.Language!='da'].duid))\n",
    "after_dup = after_dup[~after_dup.duid.isin(out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d92d86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:15:27.288010Z",
     "iopub.status.busy": "2022-09-29T13:15:27.287839Z",
     "iopub.status.idle": "2022-09-29T13:15:37.473564Z",
     "shell.execute_reply": "2022-09-29T13:15:37.472831Z",
     "shell.execute_reply.started": "2022-09-29T13:15:27.287994Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "after_dup.to_csv('prep 10 - infomedia DK deduplicated.csv', index=False)\n",
    "# Note: those files with upper case are the output file\n",
    "after_dup.to_csv('INFOMEDIA DEDUPLICATED.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7ea90",
   "metadata": {},
   "source": [
    "## 11. Extract entities\n",
    "**THIS STEP IS TIME CONSUMING and requires a lot of disk space.**\n",
    "\n",
    "This step will create a folder named \"nlp_docs\" containing a lot of data. You can delete it once step 11 is done to save disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c678bf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:15:37.474592Z",
     "iopub.status.busy": "2022-09-29T13:15:37.474398Z",
     "iopub.status.idle": "2022-09-29T13:15:37.478271Z",
     "shell.execute_reply": "2022-09-29T13:15:37.477634Z",
     "shell.execute_reply.started": "2022-09-29T13:15:37.474575Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data (uncomment if you restart from here)\n",
    "# after_dup = pd.read_csv('prep 10 - infomedia DK deduplicated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c396998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:15:37.479148Z",
     "iopub.status.busy": "2022-09-29T13:15:37.478977Z",
     "iopub.status.idle": "2022-09-29T13:15:39.173660Z",
     "shell.execute_reply": "2022-09-29T13:15:39.171485Z",
     "shell.execute_reply.started": "2022-09-29T13:15:37.479131Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Files and folder setup\n",
    "if os.path.exists('nlp_docs') and os.path.isdir('nlp_docs'):\n",
    "    shutil.rmtree('nlp_docs')\n",
    "if os.path.exists('prep 11 - done_dacy') and os.path.isfile('prep 11 - done_dacy'):\n",
    "    os.remove('prep 11 - done_dacy')\n",
    "! mkdir nlp_docs\n",
    "open('prep 11 - done_dacy','w').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae7cf93-e249-4e20-ad48-7864f90c3f6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:15:39.177486Z",
     "iopub.status.busy": "2022-09-29T13:15:39.176846Z",
     "iopub.status.idle": "2022-09-29T13:15:39.187973Z",
     "shell.execute_reply": "2022-09-29T13:15:39.186855Z",
     "shell.execute_reply.started": "2022-09-29T13:15:39.177425Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recover, in case something happened\n",
    "done = set(map(int,open('prep 11 - done_dacy','r').read().split()))\n",
    "len(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc327031-b984-4b60-91b6-eed475d3f3e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-29T13:15:39.196137Z",
     "iopub.status.busy": "2022-09-29T13:15:39.195760Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract tokens THIS MIGHT BE LONG\n",
    "from spacy.tokens import DocBin\n",
    "import tqdm\n",
    "done = set(map(int,open('prep 11 - done_dacy','r').read().split()))\n",
    "fdone = open('prep 11 - done_dacy','a')\n",
    "temp_done = set()\n",
    "doc_bin = DocBin(store_user_data=True)\n",
    "for num,doc in tqdm.tqdm(list(enumerate(after_dup.full_text.fillna('').values))):\n",
    "    if num in done:\n",
    "        continue\n",
    "    if num in temp_done:\n",
    "        continue\n",
    "    #print('%d %d'%(num,len(doc)),end=' ')\n",
    "    doc = nlp(doc)\n",
    "    doc_bin.add(doc)\n",
    "    temp_done.add(num)\n",
    "    if len(doc_bin)>=500:\n",
    "        bytes_data = doc_bin.to_bytes()\n",
    "        path = 'nlp_docs/%d'%num\n",
    "        f = open(path,'wb')\n",
    "        f.write(bytes_data)\n",
    "        done.update(temp_done)\n",
    "        for i in temp_done:\n",
    "            fdone.write('%d '%i)\n",
    "            fdone.flush()\n",
    "        doc_bin = DocBin(store_user_data=True)\n",
    "        temp_done = set()\n",
    "bytes_data = doc_bin.to_bytes()\n",
    "path = 'nlp_docs/%d'%num\n",
    "f = open(path,'wb')\n",
    "f.write(bytes_data)\n",
    "done.update(temp_done)\n",
    "for i in temp_done:\n",
    "    fdone.write('%d '%i)\n",
    "    fdone.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99530d-76cc-43cc-95d6-1ad4fbd66789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = ['nlp_docs/'+i for i in sorted(os.listdir('nlp_docs/'),key=lambda x: int(x))]\n",
    "entities = []\n",
    "for filename in tqdm.tqdm(files):\n",
    "    print(filename)\n",
    "    with open(filename,'rb') as f:\n",
    "        bytes_data = f.read()\n",
    "        doc_bin = DocBin().from_bytes(bytes_data)\n",
    "        parsed = list(doc_bin.get_docs(nlp.vocab))\n",
    "        #docs+=parsed\n",
    "        \n",
    "    for doc in parsed:\n",
    "        ents = []\n",
    "        for ent in doc.ents:\n",
    "            ents.append((ent.text,ent.label_))\n",
    "        entities.append(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6a06b-c5dd-415c-9d5e-e03ccf962cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "json.dump(entities, open('prep 11 - dacy entities.js','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd7ad3-9854-4e9e-ae10-a4ac626de17e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 12. Gather and clean named entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5f883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data (uncomment if you restart from here)\n",
    "# after_dup = pd.read_csv('prep 10 - infomedia DK deduplicated.csv')\n",
    "# entities = json.load(open('prep 11 - dacy entities.js','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf11fa-f7cf-4dbc-a1fa-7374f35c4d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = set(['CARDINAL','DATE','TIME','MONEY','PERCENT','ORDINAL'])\n",
    "entities = [[(i,j) for i,j in doc if not j in out] for doc in entities]\n",
    "def post_clean(ent):\n",
    "    ent = ent.strip('\"').split('\"')[0]\n",
    "    return ent.rstrip('-').rstrip('.').lstrip(',.-').strip('\"')\n",
    "entities2 = [[post_clean(i) for i,j in ents] for ents in entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b163a9-57f9-4563-b3af-2f44617373a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(entities,  open('prep 12 - entities.js','w'))\n",
    "json.dump(entities2, open('prep 12 - entities postcleaned.js','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e26bfd",
   "metadata": {},
   "source": [
    "## 13. Extract named entities and build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97f5e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data (uncomment if you restart from here)\n",
    "# after_dup = pd.read_csv('prep 10 - infomedia DK deduplicated.csv')\n",
    "# entities = json.load(open('prep 12 - entities.js','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b1002-dd18-4002-8d30-b95a3f7d8157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect\n",
    "from collections import Counter\n",
    "def post_clean(ent):\n",
    "    ent = ent.strip('\"').split('\"')[0]\n",
    "    return ent.rstrip('-').rstrip('.').lstrip(',.-').strip('\"')\n",
    "c = Counter()\n",
    "types = Counter()\n",
    "c2 = Counter()\n",
    "for ents in entities:\n",
    "    for e,typ in ents:\n",
    "        e = post_clean(e)\n",
    "        if len(e)==0:\n",
    "            continue\n",
    "        types[typ]+=1\n",
    "        c[e]+=1\n",
    "        c2[e.lower()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9ce38-de94-42d8-ab58-0681d04b7d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the types (monitoring)\n",
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1d981-1933-4680-bc0a-78f17b9d6443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove duplicates from different spellings and lowercasing\n",
    "g = nx.Graph()\n",
    "for e in c:\n",
    "    j = e.lower()\n",
    "    \n",
    "    if e[0].isupper():\n",
    "        if c2[j]>c[e]:\n",
    "            g.add_edge(j,e)\n",
    "\n",
    "e2e = {}\n",
    "for e in c:\n",
    "    e2 = e.lower()\n",
    "    if g.has_node(e2):\n",
    "        ent = list(g[e2].keys())[0]\n",
    "        e2e[e]=ent\n",
    "def resolve_ent(e):\n",
    "    if e in e2e:\n",
    "        return e2e[e]\n",
    "    return e\n",
    "print(len(g),len(e2e))\n",
    "del c,c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43863895-97d6-4caa-9e91-a0552afd54c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# More cleaning\n",
    "def post_clean(ent):\n",
    "    ent = ent.strip('\"').split('\"')[0]\n",
    "    return ent.rstrip('-').rstrip('.').lstrip(',.-').strip('\"')\n",
    "c = Counter()\n",
    "for ents in tqdm.tqdm(entities):\n",
    "    for e,typ in ents:\n",
    "        e = post_clean(e)\n",
    "        if len(e)==0:\n",
    "            continue\n",
    "        e = resolve_ent(e)\n",
    "        c[e]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d38b5-fdba-4a0d-a6c6-b388e99b2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Threshold entities (keep)\n",
    "cut = 10\n",
    "keep = set([i for i in c if c[i]>cut])\n",
    "print(\"Keep:\", len(keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b666a3-9141-4049-ac08-23536aab0015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check matches and missings\n",
    "ent_docs = []\n",
    "matches = 0\n",
    "missing = 0\n",
    "for i in tqdm.tqdm(range(len(after_dup))):\n",
    "    text = '%s'%(after_dup.iloc[i].text)\n",
    "    ents = entities[i]\n",
    "    ents = set([e for e,typ in ents])\n",
    "    e_formats = set()\n",
    "    for e in sorted(ents,key=lambda x: len(x),reverse=True):\n",
    "        e_r = post_clean(e)\n",
    "        if len(e_r)==0:\n",
    "            continue\n",
    "        e_r = resolve_ent(e_r)\n",
    "        e_format = e_r.replace(' ','_')\n",
    "        ci = text.count(e)\n",
    "        matches+=ci\n",
    "        e_formats.add(e_format)\n",
    "#        if text.count(e)<1:\n",
    "#            print(e,'missing')\n",
    "#            print(1+'2')\n",
    "#            break\n",
    "        text = text.replace(e,e_format)\n",
    "    doc = nltk.word_tokenize(text)\n",
    "    doc = [post_clean(i).lower().strip('\"') if not i in e_formats else post_clean(i) for i in doc]\n",
    "    missing+=len(set(e_formats)-(set(doc)))\n",
    "    ent_docs.append(doc)\n",
    "print(\"Missing:\",missing)\n",
    "print(\"Matches:\", matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6e5db-1e33-46da-9c9c-09feef8b5c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count w\n",
    "c_e = Counter()\n",
    "for doc in ent_docs:\n",
    "    for w in doc:\n",
    "        c_e[w]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d32c3-15a7-457e-b00f-215363a14d72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ent_docs2 = []\n",
    "for ents in tqdm.tqdm(entities):\n",
    "    temp = []\n",
    "    for e,typ in ents:\n",
    "        e = post_clean(e)\n",
    "        if len(e)==0:\n",
    "            continue\n",
    "        e = resolve_ent(e)\n",
    "        temp.append(e)\n",
    "    ent_docs2.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8953f-3450-45b2-af8e-6574931b560b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_docs = ent_docs+ent_docs2\n",
    "import random\n",
    "random.shuffle(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc76ff8-3868-44e3-a8e6-227db9a3d8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save word2vec entities\n",
    "from importlib import reload\n",
    "import run_w2vec as W2V\n",
    "W2V = reload(W2V)\n",
    "ent2v = W2V.run_w2vec(all_docs,phrases=False,emb_size=128)\n",
    "import pickle\n",
    "pickle.dump(ent2v, open('prep 13 - word2vec entities.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed808b-07f7-4c51-9e78-6f4dcab9b632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "e2typ = {e:Counter() for e in keep}\n",
    "edges = []\n",
    "for ents in tqdm.tqdm(entities):\n",
    "    for e,typ in ents:\n",
    "        e = post_clean(e)\n",
    "        if len(e)==0:\n",
    "            continue\n",
    "        e = resolve_ent(e)\n",
    "        if not e in keep:\n",
    "            continue\n",
    "        e2typ[e][typ]+=1\n",
    "    ents = [resolve_ent(post_clean(i)) for i,_ in ents]\n",
    "    ents = [i for i in ents if len(i)>0]\n",
    "    ents2 = []\n",
    "    seen = set()\n",
    "    for e in ents:\n",
    "        if not e in seen:\n",
    "            ents2.append(e)\n",
    "    ents = [post_clean(i) for i in ents2 if i in keep]\n",
    "    for i in range(len(ents)-1):\n",
    "        n = ents[i]\n",
    "        for j in range(i+1,len(ents)):\n",
    "            n2 = ents[j]\n",
    "            edges.append(tuple(sorted([n,n2])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0115a-5cfa-497d-8290-ebbc4172766d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "e2typ = {e:e2typ[e].most_common(1)[0][0] for e in e2typ}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fd15c-c853-4ada-872b-d214b2dedb37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_docs = len(entities)\n",
    "from collections import Counter\n",
    "edge_c = Counter(edges)\n",
    "pmis = {}\n",
    "import numpy as np\n",
    "alpha = 5 # smoothing term\n",
    "for edge,count in edge_c.items():\n",
    "    n,n2 = edge\n",
    "    p = (c[n]+alpha)/n_docs\n",
    "    p2 = (c[n2]+alpha)/n_docs\n",
    "    m = count/n_docs\n",
    "    pmis[edge] = m/(p*p2)\n",
    "\n",
    "pmis = Counter(pmis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a605e-f8f3-42b8-bdf2-c6b1b9770abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# W2vec distance\n",
    "import tqdm\n",
    "edge2dist = {}\n",
    "error= 0\n",
    "for n,n2 in tqdm.tqdm(pmis):\n",
    "    try:\n",
    "        dist = ent2v.wv.distance(n,n2)\n",
    "    except:\n",
    "        error+=1\n",
    "        dist = np.nan\n",
    "    edge2dist[tuple(sorted([n,n2]))] = dist\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a345e-8728-4fed-9011-1ca1c83230de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "topn = 100000\n",
    "for edge,pmi in pmis.most_common(topn):\n",
    "    n,n2 = edge\n",
    "    dist = edge2dist[edge]\n",
    "    count = edge_c[edge]\n",
    "    t,t2 = e2typ[n],e2typ[n2]\n",
    "    g.add_node(n,**{'label':t,'n_docs':c[n]})\n",
    "    g.add_node(n2,**{'label':t2,'n_docs':c[n2]})\n",
    "    g.add_edge(n,n2,**{'w2vec_dist':dist,'pmi':pmi,'count':count})\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be61d03-8e12-4c2c-81aa-9ad0923deffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "part = community_louvain.best_partition(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1ccc5-2f49-4512-a6bb-5c78e98ed3af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate community degree to weigh labels\n",
    "com2n = {p:[] for p in part.values()}\n",
    "for n,p in part.items():\n",
    "    g.nodes[n]['community'] = str(p)\n",
    "    com2n[p].append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fee4c9-39a3-4f21-888c-2b8ba2d3e08d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for p,nodes in com2n.items():\n",
    "    degs = np.array([len(g[n]) for n in nodes])\n",
    "    ma = max(degs)\n",
    "    m = np.mean(degs)\n",
    "    degs_sqrt = np.sqrt(degs)\n",
    "    rel_deg = degs/ma\n",
    "    for n,rel_d in zip(nodes,rel_deg):\n",
    "        g.nodes[n]['relative_degree'] = rel_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eff52b-2bfd-4d34-9e98-bd3400edb3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save network\n",
    "nx.write_graphml(g,'INFOMEDIA NER PMI NETWORK.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84ffac-c02a-4164-be32-494103938109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save entities\n",
    "keep_entities = set(g)\n",
    "pickle.dump(keep_entities, open('prep 13 - final_entities.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b2706-e3e5-4991-8ad3-aefdb9f2d747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dat = []\n",
    "for num,ents in enumerate(entities):\n",
    "    seen = set()\n",
    "    doc = after_dup.iloc[num].duid\n",
    "    for e,typ in ents:\n",
    "        e = post_clean(e)\n",
    "        e = resolve_ent(e)\n",
    "        if not e in seen:\n",
    "            d = {'doc_id':doc,'entity':e,'type':typ,'in_network':e in keep_entities}\n",
    "            dat.append(d)\n",
    "            seen.add(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee34b4-798b-4178-8a8f-43f096c4393a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ent_df = pd.DataFrame(dat)\n",
    "ent_df.to_csv('INFOMEDIA ENTITIES.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a7e90",
   "metadata": {},
   "source": [
    "## That's it\n",
    "If it worked until there, you're done, congratulations!\n",
    "\n",
    "The output files you probably want to look at are the files in upper case starting with \"INFOMEDIA\". Those are:\n",
    "* **INFOMEDIA DEDUPLICATED.csv** (the deduplicated list of entities to ingest in Elastic Search)\n",
    "* **INFOMEDIA ENTITIES.csv** (the list of entities, where they appear, and whether or not we kept them in the network)\n",
    "* **INFOMEDIA NER PMI NETWORK.gexf** (the network of entities connected by co-occurrence, weighted by positive PMI score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
